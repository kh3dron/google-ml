-	BASIC TERMS
	⁃	Features - labeled parts of data
	⁃	Labels - what we're predicting, output
	⁃	Regression v. classification: continuous v. discrete outputs
-	DESCENDING INTO ML
	⁃	y' = b + w1x1 + w2x2 ...
	⁃	b - bias
	⁃	w1 - weight 
	⁃	Loss function - calculate the diference in your model 
	⁃	MSE - mean squared error, popoular loss function
-	REDUCING LOSS
	⁃	Gradient Descent - move along the derivative of weights towards less loss
	⁃	Learning rate - how much to move the loss function
	⁃	too high and we overshoot / oscilate, too low and we waste time
	⁃	Batch - the amount of examples used in one gradient update
	⁃	Full-batch is expensive, so use less
	⁃	mini batch - use less than the whole batch
	⁃	Stochastic gradient descent - batch size of 1
	⁃	If the data is consistant - IE like a line - small batches accelerate training with little impact on performance
	⁃	Convergence - when the loss function (loss per iteration) evens out, IE model is becoming no more accurate
	⁃	Scaling - some values are better reduced (prices /= 1000)
-	WORKING WITH TENSORFLOW
	- 	notes in .py files
	⁃	Correlation Matrix: Show how much a given feature correlates with the label
-	GENERALIZATION
	⁃	Epochs - number of simulations
	⁃	enough so that the model converges
	⁃	IF no convergence, increase epochs OR learn rate
	⁃	Batch size: More for accuracy, less for efficiency
	⁃	Learning Rate: too high will cause oscilation, too low will barely move
	⁃	Defining Synthetic features: use logic to generate new columns
	⁃	simple features aren’t always enough - generate new features to train with
	⁃	Based on the loss value, this is better, but otherwise not really
-	TRAINING & TESTING SETS
	⁃	Training set v. Test set: pretty much as they sound 
	⁃	Never train on the test set
	⁃	tune the right amound of total data for the training set - hard to get just right
-	VALIDATION SET
	⁃	basically another seperate test set
	- 	When you change the model, it's good to switch up the training data too
	-	Train on train data, tune to validation set, finally test on test
	-	Exercise:
		-	Remember scale factors for some datasets: IE house value/1000
		-	The loss functions differ in the validation and training sets. Examine them and find they're sorted weirdly
		-	The solution: shuffle the dataset before cutting it into parts. 
		-	Notice now that the validation set can be smaller - 15% of the training data. 
-	REPRESENTATIONS
	-	"Feature engineering" - turning data into feature vectors usable by models
		-	Vsualize, debug, monitor
	-	Feature vectors - raw data into list of numbers
		-	Choose features which appear often - UIDs are bad
		-	Values that make sense - human readability always helps. Unix time sux
		-	No magic values: -1 for invalids are bad for the math
		-	Mapping categorical features: one-hot encoding, string of 0s with a 1 for the selected value
			-	Multi-hot encoding has multiple 1's
			-	Sparse representation: does not store the extra zeroes for one-hot encoding
		-	Account for upstream instability: avoid features which may be later redefined
	-	Cleaning data	
		-	Scaling: adjust vectors to 0->1 or -1->1, aim to roughly normalize vectors
			-	NaN trap: a NaN value will corrupt other features by multiplying/dividing
		-	Outliers: you can log every value, or clip the tails
		-	Binning: remove some noise from continuous features (latitude)
	-	Scrubbing
		-	Stats come in to play here: mean, stddev, max/min
-	FEATURE CROSSES
	-	Allows a linear model to make non-linear predictions
	-	Synthetic feature: a feature comprised of other features
	-	Crossing two one-hot encodings will increase the resultant vector size: make smart choices with bins
	-	Exercise:
		-	Creating a synthetic feature of crossed latitude/longitude buckets for housing dataset: smart
-	REGULARIZATION: SIMPLICITY
	-	Too many synthetic features can be a type of overfitting
	-	Punish models that get complex 
		-	Encourages unhelpful features to be weighted near 0 (nice)
	-	Minimize the sum of loss and commplexity
	-	L2 regularization: sum of squares of weights
	-	Generalization curve: validation and training loss as f(iterations)
	-	Lambda: the emphasis on simplicity v. loss. Higher lambda = tighter distribution of weights
-	LOGISTIC REGRESSION
	-	Used for calculating probabilities: results must be in a specific range
	-	Sigmoid function: logistic function, limits to 0 and 1 with a crossover 
	-	Use a new loss function: loss approaches infinity as prediction approaches 1 or 0
	-	Regularization matters greatly here - L2 regularization is gr8
-	CLASSIFICATION
	-	Thresholding: collapsing a logistic regression (probabalistic) into a binary state
	-	True/false positive/negative: 
		-	T/F refers to correct or incorrect prediction
		-	P/N refers to model's prediction
		-	Accuracy: not a perfect metric for Class imbalanced sets. (IE labels are not evenly discributed)
			-	Coin toss is class balanced, cancer diagnosis is imbalanced
	-	Precision: TP/(TP+FP), how confident is a "flagged true" diagnosis?
	-	Recall: TP/(TP+FN), how many true diagnosis were flagged?
		-	Improving precision often costs recall, and the inverse
		-	Higher classification threshold means better precision, lower recall
	-	ROC curve
		-	Reciever Operating Characteristic curve
		-	X is TPR (TP/TP+FN), Y is FPR (FP/FP+TN). Graph every point while moving threshold 0-1
	-	AUC curve: Area Under ROC curve
		-	effectively the probability that random positive is rated higher than a random negative sample
		-	Always-right model has AUC of 1, always wrong has 0
		-	Attrributes: (sometimes good, somethimes not)
			-	Scale invariant: only comparative
			-	Classification threshold invariant: factors in the sum of all thresholds. 
	-	Prediction Bias
		-	Average of predictions should be similar to average of observations
		-	Don't fix by adding a calibration layer - that's the symptom, not the cause
		-	Measure this by bucketing a bunch of labeled data and measuring the prediction bias
			-	Bucketing: take 1000 examples, average label of .6, measure average prediction
	-	Exercise
		-	Normalizing with Z-scores: translates every feature into a # of standard deviations from the mean
-	REGULARIZATION: SPARSITY
	-	Sparse data has lots of zeroes - NLP is here
	-	L1 regularization: minimize sum of abs value of weights 
	-	L0 regularization: minimize the number of non-zero weights (slightly different, not as used)
